{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Utilities.plot import histvstarget, distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from the data exploration notebook that we now have three different data sets now, the original, one with PCA and one with attribute selection using BFE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets take 10% of the data out to be used as a validation set and create the three different training-sets from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingdata_org = pd.read_csv('input/Train.csv')\n",
    "cols = trainingdata_org.columns\n",
    "trainingdata_org = trainingdata_org.dropna(axis=0)\n",
    "ground_truth = trainingdata_org['Outcome']\n",
    "trainingdata_org = trainingdata_org.drop(['Outcome'], axis=1)\n",
    "x_train_un, x_val_un, y_train, y_val = train_test_split(trainingdata_org, ground_truth, \n",
    "                                                        test_size=0.1, random_state = 22)\n",
    "\n",
    "meantrain = np.mean(x_train_un)\n",
    "stdtrain = np.std(x_train_un)\n",
    "standardize = lambda x: ((x - meantrain)/stdtrain)\n",
    "invstandardize = lambda x: ((x*stdtrain + meantrain))\n",
    "\n",
    "x_train = standardize(x_train_un)\n",
    "x_val = standardize(x_val_un)\n",
    "\n",
    "trainingdata = pd.DataFrame(np.hstack((x_train, y_train[:,np.newaxis])), columns = cols)\n",
    "validationdata = pd.DataFrame(np.hstack((x_val, y_val[:,np.newaxis])), columns = cols)\n",
    "trainingdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset with PCA\n",
    "import Utilities.mypca as PCA\n",
    "pca = PCA.MyPCA()\n",
    "pca.fit(x_train)\n",
    "PCA_x_train = pca.fit_transform(5, x_train)\n",
    "PCA_x_val = pca.fit_transform(5, x_val)\n",
    "print(PCA_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BFE_cols = [\"Glucose\", \"Pregnancies\", \"BMI\", \"SkinThickness\", \"Insulin\", \"BloodPressure\"]\n",
    "BFE_x_train = trainingdata[BFE_cols]\n",
    "BFE_x_val = validationdata[BFE_cols]\n",
    "print(BFE_x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models with K-fold cross validation using accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is small the best way to approach it is probably by using shallow learners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets establish a baseline for the problem by using cross validation and a few classification models. If we use k-fold cross validation since our dataset is quite small a splitting of our data into to few folds could introduce a substantial bias. On the other hand if we chose k to large we will have a lot of variance. With the small data-set in the back of our head we chose k to be moderately large, 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "GB = GaussianNB() # Gaussian NB since we have continous data.\n",
    "KN = KNeighborsClassifier(n_neighbors=4, p = 2) # 4 Neighbors by euclidian distance.\n",
    "DT_GINI = DecisionTreeClassifier(criterion=\"gini\",max_depth=4) # Decision Tree with Gini impurity for quality of split\n",
    "DT_IG = DecisionTreeClassifier(criterion=\"entropy\",max_depth=4) # Information gain for quality of split\n",
    "SV = SVC() # Support vector machine classifier.\n",
    "\n",
    "modelnames = [\"Logistic regression\",\"Gaussian Naive Bayes\", \"4-Neighbors\",\"Decisiontree Gini\",\n",
    "              \"Decisiontree Information gain\", \"Support vector machine\"]\n",
    "\n",
    "models = zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV])\n",
    "results_PCA = []\n",
    "results_BFE = []\n",
    "results = []\n",
    "\n",
    "for name,model in models:\n",
    "    kfold = KFold(n_splits=18)\n",
    "    cv_result_PCA = cross_val_score(model, PCA_x_train, y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    cv_result_BFE = cross_val_score(model, BFE_x_train, y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    cv_result = cross_val_score(model, x_train, y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    results_PCA.append(cv_result_PCA)\n",
    "    results_BFE.append(cv_result_BFE)\n",
    "    results.append(cv_result)\n",
    "\n",
    "print(\"PCA\")\n",
    "for name, res in zip(modelnames, results_PCA):\n",
    "    print(name,res.mean())\n",
    "print()\n",
    "print(\"BFE\")\n",
    "for name, res in zip(modelnames, results_BFE):\n",
    "    print(name,res.mean())\n",
    "print()\n",
    "print(\"All Data\")\n",
    "for name, res in zip(modelnames, results):\n",
    "    print(name,res.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "plt.subplot(3, 1, 1)\n",
    "ax = sns.boxplot(data=results_PCA)\n",
    "ax.set_title(\"PCA\")\n",
    "ax.set_xticklabels(modelnames)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "ax = sns.boxplot(data=results_BFE)\n",
    "ax.set_title(\"BFE\")\n",
    "ax.set_xticklabels(modelnames)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "ax = sns.boxplot(data=results)\n",
    "ax.set_xticklabels(modelnames)\n",
    "ax.set_title(\"Raw data\")\n",
    "#plt.savefig('modelsvsdata.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no sign of significantly better accuracy for any of the data sets, different models perform differently across the datasets. Logistic regression seems to be the best performing model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrixes and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since k-fold-cross validation did not give us any clear information, lets have a look at the confusion matrixes and accuracy on the validation data for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "models = zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV])\n",
    "\n",
    "i=1\n",
    "xlabel = [\"Predicted 0\",\"Predicted 1\"]\n",
    "ylabel = [\"True 0\",\"True 1\"]\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(PCA_x_train, y_train)\n",
    "    predictions=model.predict(PCA_x_val)\n",
    "    conf_PCA = (confusion_matrix(y_val,predictions))\n",
    "    plt.subplot(3, 6, i)\n",
    "    sns.heatmap(conf_PCA, annot=True, xticklabels=xlabel, yticklabels=ylabel)    \n",
    "    acc = (accuracy_score(y_val,predictions))\n",
    "    plt.title(name + \"\\n PCA \" + '{0:.{1}f}'.format(acc, 3))\n",
    "    model.fit(BFE_x_train, y_train)\n",
    "    predictions=model.predict(BFE_x_val)\n",
    "    conf_BFE = (confusion_matrix(y_val,predictions))\n",
    "    plt.subplot(3, 6, i+6)\n",
    "    sns.heatmap(conf_BFE, annot=True, xticklabels=xlabel, yticklabels=ylabel)  \n",
    "    acc = (accuracy_score(y_val,predictions))\n",
    "    plt.title(\"BFE \" + '{0:.{1}f}'.format(acc, 3))\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions=model.predict(x_val)\n",
    "    conf = (confusion_matrix(y_val,predictions))\n",
    "    plt.subplot(3, 6, i+12)\n",
    "    sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)  \n",
    "    acc = (accuracy_score(y_val,predictions))\n",
    "    plt.title(\"All Data \" + '{0:.{1}f}'.format(acc, 3))\n",
    "    i = i + 1\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('confusion.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that, apparently almost all models predict the unseen validation badly. The model that classifies the validation set significantly better is the decision trees. As we inspect the confusion matrix we note that most of the models have almost identical predictions when it comes to instances with true negative outcomes. Where they differ and where some of the the decision trees is superior is when we try to predict true positive outcomes. Many of the models are worse than random guessing. So why is this? It's called the False positive paradox. Basically since the majority of the instances has outcome not diabetes our models will favor predicting not diabetes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of training instances with outcome diabetes\", np.count_nonzero(y_train))\n",
    "print(\"Number of training instances with outcome not diabetes\", len(y_train)-np.count_nonzero(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to another paradox, namely the accuracy paradox. Basically it means that because of the inbalance in outcomes predictive models with a given certain accuracy might have greater predictive capability than a model with higher accuracy. For example, a model with a 1:10 ratio between positive and negative outcomes, say 15 and 150. Predicting everything as negative gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_train = [1 if i%10==0 else 0 for i in range(100)]\n",
    "prediction = [0 for i in range(len(example_train))]\n",
    "print(\"An accuracy when 10:1 ratio of\", accuracy_score(example_train,prediction))\n",
    "\n",
    "example_train = [1 if i < 102 else 0 for i in range(102+213)]\n",
    "prediction = [0 for i in range(len(example_train))]\n",
    "print(\"An accuracy when 2:1 ratio (our case) of\", accuracy_score(example_train,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our completely useless models with zero predictive power have 90% respectively 68% accuracy. In our data the inbalance is not as sever but the moral remains. Hence we exchange accuracy as metric in favor of AUC - The area under the ROC (receiver operating characteristic) curve. The ROC curve is the true positive rate (TPR) against the false positive rate (FPR) at various thresholds/ranks for the instances. The area under the curve measures discrimination, the ability to correctly classify those with and without diabetes in our case. An simple interpretation is the following: consider if we randomly draw one person that has diabetes and one without, the person with high ranking (or low) should be the one with diabetes. The area under the curve is the percentage of randomly drawn pairs for which this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"PCA AUC\")\n",
    "for name, model in zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV]):\n",
    "    model.fit(PCA_x_train, y_train)\n",
    "    predictions=model.predict(PCA_x_val)\n",
    "    roc = (roc_auc_score(y_val,predictions))\n",
    "    print(name, roc)\n",
    "    \n",
    "print(\"\\nBFE AUC\")\n",
    "for name, model in zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV]):\n",
    "    model.fit(BFE_x_train, y_train)\n",
    "    predictions=model.predict(BFE_x_val)\n",
    "    roc = (roc_auc_score(y_val,predictions))\n",
    "    print(name, roc)\n",
    "\n",
    "print(\"\\nAll data AUC\")\n",
    "for name, model in zip(modelnames,[LR, GB, KN, DT_GINI, DT_IG, SV]):\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions=model.predict(x_val)\n",
    "    roc = (roc_auc_score(y_val,predictions))\n",
    "    print(name, roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Inspecting the confusion matrixes and AUC scores we note that the absolutely best model is the decision tree with gini impurity or entropy and using all the data, hence that is the model we'll move on with. This is good in the sense that interpretability is available, somethinng that might be important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As familiar decision trees have high variance. As such we now look at the effect of increasing the maximum depth of the tree and at the same time inspecting the deviation of 100 trees at that depth. The maximum leaf nodes is fixed to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def exploredepth(cr, max_leaf_nodes):\n",
    "    depth = np.arange(1,10,1)\n",
    "    t = []\n",
    "    v = []\n",
    "    stdt = []\n",
    "    stdv = []\n",
    "    conf = []\n",
    "    falpos = []\n",
    "    falneg = []\n",
    "    for i in depth:\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        conf = []\n",
    "        conf2 = []\n",
    "        for j in range(1,100):\n",
    "            model = DecisionTreeClassifier(criterion=cr, splitter='best', \n",
    "                                       max_depth=i, min_samples_split=2, min_samples_leaf=1, \n",
    "                                       min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                                       max_leaf_nodes=max_leaf_nodes, min_impurity_split=1e-07, \n",
    "                                           class_weight=None, presort=False)\n",
    "            model.fit(x_train, y_train)\n",
    "            val_loss.append((roc_auc_score(y_val,model.predict(x_val))))\n",
    "            train_loss.append((roc_auc_score(y_train,model.predict(x_train))))\n",
    "            c = confusion_matrix(y_val,model.predict(x_val))\n",
    "            conf.append(c[1][0])\n",
    "            conf2.append(c[0][1])\n",
    "        falpos.append(np.mean(conf))\n",
    "        falneg.append(np.mean(conf2))\n",
    "        t.append(np.mean(train_loss))\n",
    "        stdt.append(np.std(train_loss))\n",
    "        v.append(np.mean(val_loss))\n",
    "        stdv.append(np.std(val_loss))\n",
    "    lw = 2\n",
    "    plt.plot(depth,t,color=\"darkorange\")\n",
    "    tp =plt.fill_between(depth, np.array(t) - np.array(stdt),\n",
    "                     np.array(t) + np.array(stdt), alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    plt.plot(depth,v,color = \"navy\")\n",
    "    vp = plt.fill_between(depth, np.array(v) - np.array(stdv),\n",
    "                     np.array(v) + np.array(stdv), alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "    fp = plt.scatter(depth, np.array(falpos)/10.0, color=\"red\")\n",
    "    fn = plt.scatter(depth, np.array(falneg)/10.0, color=\"green\")\n",
    "    plt.legend((tp,vp,fp,fn),\n",
    "               ['validation AUC','training AUC','Number of false negatives (1e-1)','Number of false positives (1e-1)']\n",
    "               , loc='lower left')\n",
    "    plt.xlabel('Depth')\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('entropy')\n",
    "exploredepth('entropy', 25)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('gini')\n",
    "exploredepth('gini', 25)\n",
    "#plt.savefig('xploredepth.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot does not imply any overfitting, by inspection the best model is the decision tree with entropy criterion using a leaf node bound of 25, and depth bound of 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a more rigorus parameter search we're gonna search an selected subset of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import zero_one_loss, f1_score\n",
    "from Utilities.utils import gridsearch\n",
    "\n",
    "model = DecisionTreeClassifier() # Our model\n",
    "param_grid = {\"max_depth\": np.arange(2,8,1), # Maximum depth of tree\n",
    "              \"max_features\": np.arange(3,8,1), # Number of features to consider when looking for the best split\n",
    "              \"max_leaf_nodes\": np.arange(4,27,1), # Maximum number of leaves in our tree.\n",
    "              \"criterion\": [\"gini\", \"entropy\"], # Splitting criteria\n",
    "              \"class_weight\": [None, 'balanced',{0: 1.105, 1: 1.15}] # Weights associated with classes.\n",
    "            }\n",
    "\n",
    "metric = roc_auc_score # Metric to use\n",
    "tiebreaker = zero_one_loss # Tie breaker metric.\n",
    "n_best_grids = 10 # 5 best grids\n",
    "\n",
    "best_score, best_grid, tiebreaker = gridsearch(model, x_train, y_train, x_val, y_val, param_grid, metric, \n",
    "                                               n_best_grids, loss=False, tiebreaker=tiebreaker)\n",
    "\n",
    "        \n",
    "for a,t,g in zip(best_score, tiebreaker, best_grid):\n",
    "    print(\"AUC:\",a) \n",
    "    print(\"Tie\",t)\n",
    "    print(\"Grid:\",g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.set_params(**best_grid[0])\n",
    "model.fit(x_train,y_train)\n",
    "c = confusion_matrix(y_val,model.predict(x_val))\n",
    "sns.heatmap(c, annot=True, xticklabels=xlabel, yticklabels=ylabel) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good result and we could naively choose this single tree, or any of the top 10 combinations as they have the same error, as our model. The problem is that every time we run the parameter search another combination of parameters will be the best tree because of the intrinsic variance of decision trees, additionally these trees will have a really good error as well because of the natural low bias of decision trees. Remember that the trees are tuned to the validation set and might, probably wont, generalize good to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is some pattern to what sort of combinations work for the problem. For the entropy criterion trees the best performing ones the maximum depth is between 5 and 7, the maximum number of leaves between 22 and 26, the maximum number of features to consider on each split around 7 and using balanced weighting for the classes. The balanced weighting adjust the weights for the classes inversely proportional to class frequencies, basically counteracting the false positive hypothesis mentioned earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gini criterion trees the best performing ones the maximum depth is around 7, the maximum number of leaves varies a lot but tend to approach higher values (≈25), the maximum number of features to consider on each split around 7 and using the proposed weighting for the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decrease variance and try to make our model to generalize we use need to use an ensemble. The first that comes to mind is trying Bootstrap aggregating or a random forest. It should theoretically provide the stability we need and reduces variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "base = DecisionTreeClassifier(criterion='entropy', splitter='best', \n",
    "                                       max_depth=5, min_samples_split=2, min_samples_leaf=1, \n",
    "                                       min_weight_fraction_leaf=0.0, max_features=7, random_state=None, \n",
    "                                       max_leaf_nodes=22, min_impurity_split=1e-07, \n",
    "                                           class_weight='balanced', presort=False)\n",
    "\n",
    "model = BaggingClassifier(base_estimator=base, n_estimators=100, max_samples=1.0, max_features=1.0, \n",
    "                  bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, \n",
    "                  n_jobs=1, random_state=None, verbose=0)\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_val)\n",
    "conf = (confusion_matrix(y_val,predictions))\n",
    "auc = (roc_auc_score(y_val,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)\n",
    "plt.title(\"AUC  \" + str(auc))\n",
    "print(auc)\n",
    "#plt.savefig('Bagging.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=5, \n",
    "                       min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                       max_features='auto', max_leaf_nodes=22, min_impurity_split=1e-07, \n",
    "                       bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \n",
    "                       warm_start=False, class_weight='balanced')\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_val)\n",
    "conf = (confusion_matrix(y_val,predictions))\n",
    "auc = (roc_auc_score(y_val,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)\n",
    "plt.title(\"AUC  \" + str(auc))\n",
    "print(auc)\n",
    "#plt.savefig('randfor.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensembles seems to provide stability and decrease the variance run to run. However they seem to introduce some bias. Looking back at when we compared entropy and gini criterion and looked at the effect of the depth and variance. Remember that the gini trees generally had few false positives while entropy trees had few false negatives. An viable hypothesis might be that the two complement each other, and because we in the ensembles above only use the one or the other. To test the hypothesis we're gonna try an VotingClassifier using the parameters search result we acquired above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting classifier consists of a 5:4 ratio of entropy and gini trees since the gini trees showed less potential in the grid search. The classification is done using a majority vote rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "mods = []\n",
    "for i in range(1,100): # 100 Trees provides low variance.\n",
    "    # A parameter combination that were sucessfull for entropy trees.\n",
    "    mods.append((str(i),DecisionTreeClassifier(criterion='entropy', splitter='best', \n",
    "                               max_depth=5, min_samples_split=2, min_samples_leaf=1, \n",
    "                               min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                               max_leaf_nodes=22, min_impurity_split=1e-07, class_weight='balanced', presort=False)))\n",
    "    if(i < 80):\n",
    "        # A parameter combination that were sucessfull for gini trees.\n",
    "        mods.append((str(i)+\"gi\",DecisionTreeClassifier(criterion='gini', splitter='best', \n",
    "                               max_depth=7, min_samples_split=2, min_samples_leaf=1, \n",
    "                               min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                               max_leaf_nodes=25, min_impurity_split=1e-07, \n",
    "                                                        class_weight={0: 1.105, 1: 1.15}, presort=False)))\n",
    "model = VotingClassifier(estimators=mods, voting='hard', n_jobs=1)\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_val)\n",
    "conf = (confusion_matrix(y_val,predictions))\n",
    "auc = (roc_auc_score(y_val,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)\n",
    "print(auc)\n",
    "print((roc_auc_score(y_train,model.predict(x_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis might be true, this is a really good separation and a AUC of nearly 89 for the validation set and 93 for the training set. Thats not a gigantic difference and hopefully the constraints on the trees have prevented the model from overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before giving the test set a go we train our model on the entire training dataset with some imputing. We also put it into a pipeline to automate the work-flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer,StandardScaler\n",
    "X = pd.read_csv('input/Train.csv')\n",
    "Y = X['Outcome']\n",
    "X = X.drop([\"Outcome\"], axis=1)\n",
    "\n",
    "model = VotingClassifier(estimators=mods, voting='hard', n_jobs=1)\n",
    "pipeline = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"standardizer\", StandardScaler()),\n",
    "                      (\"VotingClassifier\", model)])\n",
    "pipeline.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('input/Test.csv')\n",
    "test.dropna(inplace = True)\n",
    "truth = test['Outcome']\n",
    "test.drop('Outcome', axis = 1, inplace = True)\n",
    "predictions = pipeline.predict(test)\n",
    "conf = (confusion_matrix(truth,predictions))\n",
    "acc = (accuracy_score(truth,predictions))\n",
    "auc = (roc_auc_score(truth,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)  \n",
    "print(\"accuracy\", acc)\n",
    "print(\"AUC\", auc)\n",
    "#plt.savefig('test_conf.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eureka, this is really good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('input/Test.csv')\n",
    "truth = test['Outcome']\n",
    "test.drop('Outcome', axis = 1, inplace = True)\n",
    "predictions = pipeline.predict(test)\n",
    "conf = (confusion_matrix(truth,predictions))\n",
    "acc = (accuracy_score(truth,predictions))\n",
    "auc = (roc_auc_score(truth,predictions))\n",
    "sns.heatmap(conf, annot=True, xticklabels=xlabel, yticklabels=ylabel)  \n",
    "print(\"accuracy\", acc)\n",
    "print(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good, but still good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
